{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vivekn Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python decorator to measure the execution time of methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts))\n",
    "        else:\n",
    "            print('%r  %2.2f s' % \\\n",
    "                  (method.__name__, (te - ts)))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class with the main functions to be used for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "class ViveknSentiment:\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def loadData(file_path):\n",
    "        \"\"\"\n",
    "        This method loads a csv file\n",
    "        \"\"\"\n",
    "        data = spark. \\\n",
    "               read. \\\n",
    "               load(file_path,\n",
    "                    format=\"com.databricks.spark.csv\",\n",
    "                    header=\"true\") \n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def fitModel(pipeline, data):\n",
    "        model = pipeline.fit(data)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def exportModel(pipeline, data, object_type, dir_name):\n",
    "        \"\"\"\n",
    "        This method exports a model to a directory\n",
    "        :param pipeline: Pipeline or PipelineModel objects\n",
    "        :param data: Data to fit a PipelineModel\n",
    "        :param object_type: p for Pipeline, pm for PipelineModel\n",
    "        :param dir_name: directory where the model is saved to\n",
    "        \"\"\"\n",
    "        if object_type == \"p\":\n",
    "            pipeline.write().overwrite().save(dir_name)\n",
    "        elif object_type == \"pm\":\n",
    "            pipeline.fit(data).write().overwrite().save(dir_name)\n",
    "        print(f\"Model exported\")\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def loadModel(object_type, dir_name):\n",
    "        \"\"\"\n",
    "        This method loads a model from a directory\n",
    "        :param object_type: p for Pipeline, pm for PipelineModel\n",
    "        :param dir_name: directory where the model is loaded from \n",
    "        \"\"\"\n",
    "        if object_type == \"p\":\n",
    "            model = Pipeline.read().load(dir_name)\n",
    "        elif object_type == \"pm\":\n",
    "            model = PipelineModel.read().load(dir_name)\n",
    "        print(f\"Model loaded\")\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def predict(model, data):\n",
    "        prediction = model.transform(data)\n",
    "        return prediction\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def getLabeledPrediction(labeled_data, predicted_data):\n",
    "        \"\"\"\n",
    "        This method creates a dataframe with the required format\n",
    "        for evaluation of a binary classifier\n",
    "        :param labeled_data: dataset with ground truth data\n",
    "        :param predicted_data: dataset with predictions\n",
    "        \"\"\"\n",
    "        predictions = labeled_data.join(predicted_data, \n",
    "                                        labeled_data.id == predicted_data.id)\n",
    "        predictions = predictions. \\\n",
    "                      withColumn(\"prediction\", \n",
    "                                  F.when(F.col(\"finished_sentiment\") == \"result->positive\", 1). \\\n",
    "                                         otherwise(0))\n",
    "        predictions.show(5)\n",
    "        # Evaluator for binary classification, expects input column label.\n",
    "        predictions = predictions. \\\n",
    "                      select(F.col(\"sentiment\").alias(\"label\").cast(\"double\"),  \n",
    "                             F.col(\"prediction\").cast(\"double\"))\n",
    "        return predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    @timeit\n",
    "    def evaluatePrediction(predictions, metric=\"areaUnderROC\"):\n",
    "        \"\"\"\n",
    "        This method evaluates the model\n",
    "        :param predictions: dataset in the output format of getLabeledPrediction\n",
    "        :param metric: areaUnderROC or areaUnderPR\n",
    "        \"\"\"\n",
    "        # Evaluator for binary classification, expects two input columns: rawPrediction and label.\n",
    "        evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                                  metricName=metric)\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_path = \"file:///\" + os.getcwd() + \"/../../../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define the dataframe\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                    .setInputCol(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"document\"]) \\\n",
    "            .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Normalizer\n",
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spell Checker\n",
    "corpus = home_path + \\\n",
    "         \"spark-nlp/src/test/resources/spell/sherlockholmes.txt\" \n",
    "# \"spark-nlp-models/src/main/resources/spell/wiki1_en.txt\"                          \n",
    "         \n",
    "dictionary = home_path + \\\n",
    "             \"spark-nlp-models/src/main/resources/spell/words.txt\"\n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"normal\"]) \\\n",
    "            .setOutputCol(\"spell\") \\\n",
    "            .setDictionary(dictionary) \\\n",
    "            .setCorpus(corpus) \\\n",
    "            .setDoubleVariants(True) \\\n",
    "            .setCaseSensitive(True) \\\n",
    "            .setShortCircuit(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported\n",
      "'exportModel'  0.06 s\n"
     ]
    }
   ],
   "source": [
    "# Export Spell Checker\n",
    "pipeline = Pipeline(stages=[spell_checker])\n",
    "ViveknSentiment.exportModel(pipeline, \"\", \"p\", \"./sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_text = home_path + \\\n",
    "                 \"spark-nlp-models/src/main/resources/vivekn/training_positive\"\n",
    "negative_text = home_path + \\\n",
    "                 \"spark-nlp-models/src/main/resources/vivekn/training_negative\"\n",
    "    \n",
    "sentiment_detector = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"spell\", \"document\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setPruneCorpus(0) \\\n",
    "    .setPositiveSource(positive_text) \\\n",
    "    .setNegativeSource(negative_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeKeys(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'loadData'  0.07 s\n",
      "+----+--------------------+---------+\n",
      "|  id|                text|sentiment|\n",
      "+----+--------------------+---------+\n",
      "|3995|da vinci code was...|        0|\n",
      "|3996|Then again, the D...|        0|\n",
      "|3999|God, Yahoo Games ...|        0|\n",
      "|4000|Da Vinci Code doe...|        0|\n",
      "|4001|And better...-We ...|        0|\n",
      "|4002|Last time, Da Vin...|        0|\n",
      "|4003|And better...-We ...|        0|\n",
      "|4004|And better..-We a...|        0|\n",
      "|4006|If Jesus is fabri...|        0|\n",
      "|4007|I think this bols...|        0|\n",
      "+----+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file = home_path + \\\n",
    "                \"spark-nlp-models/src/main/resources/datasets/training_balanced\"\n",
    "train_data = ViveknSentiment.loadData(training_file)\n",
    "train_data.cache()\n",
    "train_data.show(10)\n",
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "'loadModel'  0.12 s\n"
     ]
    }
   ],
   "source": [
    "# Load spell checker model\n",
    "# sc = ViveknSentiment.loadModel(\"p\", \"./sc\")\n",
    "# spell_checker = sc.getStages()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fitModel'  5.29 s\n",
      "'predict'  0.06 s\n",
      "+----+--------------------+------------------+\n",
      "|  id|                text|finished_sentiment|\n",
      "+----+--------------------+------------------+\n",
      "|3995|da vinci code was...|  result->negative|\n",
      "|3996|Then again, the D...|  result->positive|\n",
      "|3999|God, Yahoo Games ...|  result->negative|\n",
      "|4000|Da Vinci Code doe...|  result->negative|\n",
      "|4001|And better...-We ...|  result->negative|\n",
      "|4002|Last time, Da Vin...|  result->negative|\n",
      "|4003|And better...-We ...|  result->negative|\n",
      "|4004|And better..-We a...|  result->negative|\n",
      "|4006|If Jesus is fabri...|  result->negative|\n",
      "|4007|I think this bols...|  result->negative|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "model = ViveknSentiment.fitModel(pipeline, train_data)\n",
    "train_predict = ViveknSentiment.predict(model, train_data)\n",
    "train_predict.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|  id|                text|finished_sentiment|\n",
      "+----+--------------------+------------------+\n",
      "|3996|Then again, the D...|  result->positive|\n",
      "|4018|and also, The Da ...|  result->positive|\n",
      "|4042|DA VINCI CODE-SUC...|  result->positive|\n",
      "|4115|The Da vinci Code...|  result->positive|\n",
      "|4149|Also, Da Vinci Co...|  result->positive|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time elapsed in query: 3.074977159500122\n"
     ]
    }
   ],
   "source": [
    "# Find positive sentiments\n",
    "start = time.time()\n",
    "train_predict.where(\"finished_sentiment == 'result->positive'\").show(5)\n",
    "end = time.time()\n",
    "print(\"Time elapsed in query: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|  id|                text|finished_sentiment|\n",
      "+----+--------------------+------------------+\n",
      "|3995|da vinci code was...|  result->negative|\n",
      "|3999|God, Yahoo Games ...|  result->negative|\n",
      "|4000|Da Vinci Code doe...|  result->negative|\n",
      "|4001|And better...-We ...|  result->negative|\n",
      "|4002|Last time, Da Vin...|  result->negative|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time elapsed in query: 5.151686668395996\n"
     ]
    }
   ],
   "source": [
    "# Find negative sentiments\n",
    "start = time.time()\n",
    "train_predict.where(\"finished_sentiment == 'result->negative'\").show(5)\n",
    "end = time.time()\n",
    "print(\"Time elapsed in query: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- finished_sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predict.printSchema()  # print data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures the training accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+----+--------------------+------------------+----------+\n",
      "|  id|                text|sentiment|  id|                text|finished_sentiment|prediction|\n",
      "+----+--------------------+---------+----+--------------------+------------------+----------+\n",
      "|3995|da vinci code was...|        0|3995|da vinci code was...|  result->negative|         0|\n",
      "|3996|Then again, the D...|        0|3996|Then again, the D...|  result->positive|         1|\n",
      "|3999|God, Yahoo Games ...|        0|3999|God, Yahoo Games ...|  result->negative|         0|\n",
      "|4000|Da Vinci Code doe...|        0|4000|Da Vinci Code doe...|  result->negative|         0|\n",
      "|4001|And better...-We ...|        0|4001|And better...-We ...|  result->negative|         0|\n",
      "+----+--------------------+---------+----+--------------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "'getLabeledPrediction'  91.71 s\n"
     ]
    }
   ],
   "source": [
    "predictions = ViveknSentiment.getLabeledPrediction(train_data, train_predict)\n",
    "# predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'evaluatePrediction'  97.20 s\n",
      "Area Under ROC: 0.9575\n"
     ]
    }
   ],
   "source": [
    "roc = ViveknSentiment.evaluatePrediction(predictions)\n",
    "print(\"Area Under ROC: {:0.4f}\".format(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'evaluatePrediction'  95.18 s\n",
      "Area Under PR: 0.9613\n"
     ]
    }
   ],
   "source": [
    "pr = ViveknSentiment.evaluatePrediction(predictions, metric=\"areaUnderPR\")\n",
    "print(\"Area Under PR: {:0.4f}\".format(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported\n",
      "'exportModel'  4.28 s\n"
     ]
    }
   ],
   "source": [
    "# Export model\n",
    "ViveknSentiment.exportModel(pipeline, train_data, \"pm\", \"./ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "'loadModel'  3.82 s\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = ViveknSentiment.loadModel(\"pm\", \"./ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'loadData'  0.07 s\n",
      "+---+--------------------+---------+\n",
      "| id|                text|sentiment|\n",
      "+---+--------------------+---------+\n",
      "|  0|The Da Vinci Code...|        1|\n",
      "|  1|this was the firs...|        1|\n",
      "|  2|i liked the Da Vi...|        1|\n",
      "| 13|The Da Vinci Code...|        1|\n",
      "| 26|I really like The...|        1|\n",
      "| 27|Da Vinci Code is ...|        1|\n",
      "| 31|And then we went ...|        1|\n",
      "| 34|Well I did enjoy ...|        1|\n",
      "| 41|And I was quite p...|        1|\n",
      "| 44|The Da Vinci Code...|        1|\n",
      "+---+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load testing data\n",
    "test_file = home_path + \\\n",
    "            \"spark-nlp-models/src/main/resources/datasets/testing\"\n",
    "test_data = ViveknSentiment.loadData(test_file)\n",
    "test_data.cache()\n",
    "test_data.show(10)\n",
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'predict'  0.07 s\n"
     ]
    }
   ],
   "source": [
    "# Predict test data\n",
    "test_predict = ViveknSentiment.predict(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+---+--------------------+------------------+----------+\n",
      "| id|                text|sentiment| id|                text|finished_sentiment|prediction|\n",
      "+---+--------------------+---------+---+--------------------+------------------+----------+\n",
      "|  0|The Da Vinci Code...|        1|  0|The Da Vinci Code...|  result->positive|         1|\n",
      "|  1|this was the firs...|        1|  1|this was the firs...|  result->positive|         1|\n",
      "|  2|i liked the Da Vi...|        1|  2|i liked the Da Vi...|  result->positive|         1|\n",
      "| 13|The Da Vinci Code...|        1| 13|The Da Vinci Code...|  result->positive|         1|\n",
      "| 26|I really like The...|        1| 26|I really like The...|  result->positive|         1|\n",
      "+---+--------------------+---------+---+--------------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "'getLabeledPrediction'  198.37 s\n"
     ]
    }
   ],
   "source": [
    "predictions = ViveknSentiment.getLabeledPrediction(test_data, test_predict)\n",
    "# predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'evaluatePrediction'  195.32 s\n",
      "Area Under ROC: 0.8263\n"
     ]
    }
   ],
   "source": [
    "roc = ViveknSentiment.evaluatePrediction(predictions)\n",
    "print(\"Area Under ROC: {:0.4f}\".format(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'evaluatePrediction'  200.07 s\n",
      "Area Under PR: 0.8977\n"
     ]
    }
   ],
   "source": [
    "pr = ViveknSentiment.evaluatePrediction(predictions, metric=\"areaUnderPR\")\n",
    "print(\"Area Under PR: {:0.4f}\".format(pr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
