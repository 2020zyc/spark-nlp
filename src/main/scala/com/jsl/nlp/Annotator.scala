package com.jsl.nlp

import org.apache.spark.ml.Transformer
import org.apache.spark.ml.param.{Param, ParamMap}
import org.apache.spark.ml.util.DefaultParamsWritable
import org.apache.spark.sql.expressions.UserDefinedFunction
import org.apache.spark.sql.{DataFrame, Dataset, Row}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions.{array, udf}

/**
  * This trait implements logic that applies nlp using Spark ML Pipeline transformers
  * Should strongly change once UsedDefinedTypes are allowed
  * https://issues.apache.org/jira/browse/SPARK-7768
  */
trait Annotator extends Transformer with DefaultParamsWritable {

  /**
    * internal types to show Rows as a relevant StructType
    * Should be deleted once Spark releases UserDefinedTypes to @developerAPI
    */
  type DocumentContent = Row
  type AnnotationContent = Seq[Row]

  /**
    * Annotator reference id. Used to identify elements in metadata or to refer to this annotator type
    */
  protected val annotatorType: String

  /**
    * other annotator types required by this annotator type
    * e.g. POS Tagger may require Sentence boundaries detected
    */
  protected var requiredAnnotatorTypes: Array[String]

  /**
    * Dataset column that contains the document
    */
  private val documentCol: Param[String] =
    new Param(this, "document column", "the input document column")

  /**
    * columns that contain annotations necessary to run this annotator
    * AnnotatorType is used both as input and output columns if not specified
    */
  private val inputAnnotationCols: Param[Array[String]] =
    new Param(this, "inputAnnotationCols", "the input annotation columns")

  private val outputAnnotationCol: Param[String] =
    new Param(this, "outputAnnotationCol", "the output annotation column")

  /**
    * takes a document and annotations and produces new annotations of this annotator's annotation type
    * @param document Usually refers to a container of raw text
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  protected def annotate(document: Document, annotations: Seq[Annotation]): Seq[Annotation]

  /**
    * takes a [[Dataset]] and checks to see if all the required annotation types are present.
    * @param dataFrame The dataframe to be validated
    * @return True if all the required types are present, else false
    */
  private def validate(dataFrame: Dataset[_]): Boolean = requiredAnnotatorTypes.forall {
    requiredAnnotationType =>
      dataFrame.schema.exists {
        field =>
          field.metadata.contains("annotationType") &&
            field.metadata.getString("annotationType") == requiredAnnotationType
      }
  }

  /**
    * Wraps annotate to happen inside SparkSQL user defined functions in order to act with [[org.apache.spark.sql.Column]]
    * @return udf function to be applied to [[inputAnnotationCols]] using this annotator's annotate function as part of ML transformation
    */
  private def dfAnnotate: UserDefinedFunction = udf {
    (docProperties: Document, aProperties: Seq[Annotation]) =>
      annotate(docProperties, aProperties)
  }

  /**
    * @return [[DataType]] providing metadata shape of [[outputAnnotationCol]]
    */
  private def outputDataType: DataType = ArrayType(Annotation.dataType)

  /** Overrides document column to be used*/
  def setDocumentCol(value: String): this.type = set(documentCol, value)

  /** @return current document column */
  def getDocumentCol: String = $(documentCol)

  /** Overrides required annotators column if different than default */
  def setInputAnnotationCols(value: Array[String]): this.type = {
    require(
      requiredAnnotatorTypes.length > 0,
      "This annotator does not require any annotations"
    )
    require(
      value.length == requiredAnnotatorTypes.length,
      s"Provided column amount differs from required amount: ${requiredAnnotatorTypes.length}"
    )
    set(inputAnnotationCols, value)
  }

  /** @return input annotations columns currently used */
  def getInputAnnotationCols: Array[String] = get(inputAnnotationCols).getOrElse(requiredAnnotatorTypes)

  /** Overrides annotation column name when transforming */
  def setOutputAnnotationCol(value: String): this.type = set(outputAnnotationCol, value)

  /** Gets annotation column name going to generate */
  def getOutputAnnotationCol: String = get(outputAnnotationCol).getOrElse(annotatorType)

  /**
    * Given requirements are met, this applies ML transformation within a Pipeline or stand-alone
    * Output annotation will be generated as a new column, previous annotations are still available separately
    * metadata is built at schema level to record annotations structural information outside its content
    * @param dataset [[Dataset[Row]]]
    * @return
    */
  override def transform(dataset: Dataset[_]): DataFrame = {
    require(validate(dataset), s"The following transformations are missing in the pipeline: ${requiredAnnotatorTypes.mkString(", ")}")
    val metadataBuilder: MetadataBuilder = new MetadataBuilder()
    metadataBuilder.putString("annotationType", annotatorType)
    dataset.withColumn(
      getOutputAnnotationCol,
      dfAnnotate(
          dataset.col($(documentCol)),
        array(getInputAnnotationCols.map(c => dataset.col(c)):_*)
      ).as(getOutputAnnotationCol, metadataBuilder.build)
    )
  }

  /** requirement for pipeline transformation validation. It is called on fit() */
  override def transformSchema(schema: StructType): StructType = {
    println(s"this is schema $schema")
    require(schema.fieldNames.contains($(documentCol)), "Document column not found. Please use setDocumentCol")
    require(schema($(documentCol)).dataType == Document.dataType,
      s"column [${$(documentCol)}] must be an NLP Document column.")
    getInputAnnotationCols.foreach {
      annotationColumn =>
        require(getInputAnnotationCols.forall(schema.fieldNames.contains),
          s"pipeline annotator stages incomplete. " +
            s"expected: ${getInputAnnotationCols.mkString(", ")}, " +
            s"found: ${schema.fields.filter(_.dataType == Annotation.dataType).map(_.name).mkString(", ")}")
        require(schema(annotationColumn).dataType == ArrayType(Annotation.dataType),
          s"column [$annotationColumn] must be an NLP Annotation column")
    }
    if (schema.fieldNames.contains(annotatorType)) {
      throw new IllegalArgumentException(s"Schema already has an annotator of type: $annotatorType.")
    }
    val metadataBuilder: MetadataBuilder = new MetadataBuilder()
    requiredAnnotatorTypes.foreach{ requiredType => metadataBuilder.putString("annotationType", requiredType)}
    val outputFields = schema.fields :+
      StructField(annotatorType, outputDataType, nullable = false, metadataBuilder.build)
    StructType(outputFields)
  }

  /** requirement for annotators copies */
  override def copy(extra: ParamMap): Transformer = defaultCopy(extra)

}