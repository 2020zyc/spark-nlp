package com.jsl.nlp.annotators.sda

import com.jsl.nlp.annotators.common.{TaggedSentence, TaggedWord}
import com.jsl.nlp.annotators.param.AnnotatorParam
import com.jsl.nlp.annotators.{Lemmatizer, RegexTokenizer}
import com.jsl.nlp.annotators.pos.POSTagger
import com.jsl.nlp.annotators.sbd.SentenceDetector
import com.jsl.nlp.annotators.sda.pragmatic.SerializedScorerApproach
import com.jsl.nlp.{Annotation, Annotator, Document}
import org.apache.spark.ml.util.{DefaultParamsReadable, Identifiable}

/**
  * Created by saif on 12/06/2017.
  */

/**
  * Gives a good or bad score to a sentence based on the approach used
  * @param uid internal uid needed for saving annotator to disk
  * @@ model: Implementation to be applied for sentiment analysis
  */
class SentimentDetector(override val uid: String) extends Annotator {

  /** Uses annotatorParam for custom serialization */
  val model: AnnotatorParam[SentimentApproach, SerializedScorerApproach] =
    new AnnotatorParam[SentimentApproach, SerializedScorerApproach](
      this,
      "Sentiment detection model",
      "Approach to translate into expressed sentiment"
    )

  override val annotatorType: String = SentimentDetector.annotatorType

  /** Requires sentence boundaries to give score in context
    * Tokenization to make sure tokens are within bounds
    * Transitivity requirements are also required
    */
  override var requiredAnnotatorTypes: Array[String] = Array(
    RegexTokenizer.annotatorType,
    SentenceDetector.annotatorType
  )

  def this() = this(Identifiable.randomUID(SentimentDetector.annotatorType))

  def getModel: SentimentApproach = $(model)

  /**
    * Model to use determines whether to require other annotators
    * @param targetModel
    * @return
    */
  def setModel(targetModel: SentimentApproach): this.type = {
    if (targetModel.requiresPOS) requiredAnnotatorTypes = requiredAnnotatorTypes :+ POSTagger.annotatorType
    if (targetModel.requiresLemmas) requiredAnnotatorTypes = requiredAnnotatorTypes :+ Lemmatizer.annotatorType
    set(model, targetModel)
  }

  /**
    * Tokens are needed to identify each word in a sentence boundary
    * POS tags are optionally submitted to the model in case they are needed
    * Lemmas are another optional annotator for some models
    * Bounds of sentiment are hardcoded to 0 as they render useless
    * @param document Usually refers to a container of raw text
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(document: Document, annotations: Seq[Annotation]): Seq[Annotation] = {
    val tokens = annotations.filter(_.annotatorType == RegexTokenizer.annotatorType)
    val sentences = annotations.filter(_.annotatorType == SentenceDetector.annotatorType)
    val tags = annotations.filter(_.annotatorType == POSTagger.annotatorType)
    val lemmas = annotations.filter(_.annotatorType == Lemmatizer.annotatorType).flatMap(_.metadata).toMap
    val taggedSentences = sentences.map(sentence => {
      val taggedWords = tags.find(tag => tag.end == sentence.end).map(_.metadata)
        .getOrElse(tokens.filter(_.end <= sentence.end).flatMap(_.metadata.values))
        .map {
          case (word: String, tag: String) => TaggedWord(lemmas.getOrElse(word, word), tag)
          case word: String => TaggedWord(word, "?NOTAG?")
        }.toArray
      TaggedSentence(taggedWords)
    }).toArray
    val score = getModel.score(taggedSentences)
    Seq(Annotation(
      SentimentDetector.annotatorType,
      0,
      0,
      Map(SentimentDetector.annotatorType -> score.toString)
    ))
  }

}
object SentimentDetector extends DefaultParamsReadable[SentimentDetector] {
  val annotatorType = "sda"
}
